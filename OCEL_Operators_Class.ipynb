{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pycelonis:Couldn't connect to IBC, trying to switch key type...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-21 21:54:18 - pycelonis: Login successful! Hello Louis. PyCelonis Version: 1.5.14\n",
      "2022-01-21 21:54:18 - pycelonis: Your key has following permissions:\n",
      "[\n",
      "    {\n",
      "        \"permissions\": [\n",
      "            \"EDIT_ALL_SPACES\",\n",
      "            \"MANAGE_PERMISSIONS\",\n",
      "            \"CREATE_SPACE\",\n",
      "            \"DELETE_ALL_SPACES\"\n",
      "        ],\n",
      "        \"serviceName\": \"package-manager\"\n",
      "    },\n",
      "    {\n",
      "        \"permissions\": [\n",
      "            \"EDIT_AGENTS\",\n",
      "            \"VIEW_AGENTS\",\n",
      "            \"REGISTER_AGENTS\",\n",
      "            \"MANAGE_PERMISSIONS\"\n",
      "        ],\n",
      "        \"serviceName\": \"workflows\"\n",
      "    },\n",
      "    {\n",
      "        \"permissions\": [\n",
      "            \"EDIT_CLIENT_SETTINGS\",\n",
      "            \"EDIT_USERS\"\n",
      "        ],\n",
      "        \"serviceName\": \"task-mining\"\n",
      "    },\n",
      "    {\n",
      "        \"permissions\": [\n",
      "            \"MANAGE_SSO_SETTINGS\",\n",
      "            \"USE_AUDIT_LOGS_API\",\n",
      "            \"MANAGE_GENERAL_SETTINGS\",\n",
      "            \"MANAGE_GROUPS\",\n",
      "            \"MANAGE_APPLICATIONS\",\n",
      "            \"MANAGE_LICENSE_SETTINGS\",\n",
      "            \"MANAGE_MEMBERS\",\n",
      "            \"MANAGE_UPLINK_INTEGRATIONS\",\n",
      "            \"MANAGE_PERMISSIONS\",\n",
      "            \"MANAGE_ADMIN_NOTIFICATIONS\",\n",
      "            \"IMPORT_MEMBERS\",\n",
      "            \"MANAGE_MEMBER_LOCKING_POLICY\"\n",
      "        ],\n",
      "        \"serviceName\": \"team\"\n",
      "    },\n",
      "    {\n",
      "        \"permissions\": [\n",
      "            \"CREATE_PROJECTS\",\n",
      "            \"MANAGE_SKILLS\",\n",
      "            \"ACCESS_ALL_PROJECTS\",\n",
      "            \"MY_INBOX\"\n",
      "        ],\n",
      "        \"serviceName\": \"action-engine\"\n",
      "    },\n",
      "    {\n",
      "        \"permissions\": [\n",
      "            \"CREATE_AND_MODIFY_CATEGORIES\",\n",
      "            \"DELETE_EXISTING_CATEGORIES\",\n",
      "            \"MODIFY_EXISTING_CATEGORIES\"\n",
      "        ],\n",
      "        \"serviceName\": \"process-repository\"\n",
      "    },\n",
      "    {\n",
      "        \"permissions\": [\n",
      "            \"CREATE_WORKSPACE\",\n",
      "            \"MOVE_TO\",\n",
      "            \"DELETE_ALL_WORKSPACES\",\n",
      "            \"DELETE_ALL_ANALYSES\",\n",
      "            \"EDIT_ALL_ANALYSES\",\n",
      "            \"EDIT_ALL_WORKSPACES\",\n",
      "            \"USE_ALL_ANALYSES\",\n",
      "            \"CREATE_ANALYSES\",\n",
      "            \"MANAGE_PERMISSIONS\",\n",
      "            \"EXPORT_CONTENT\"\n",
      "        ],\n",
      "        \"serviceName\": \"process-analytics\"\n",
      "    },\n",
      "    {\n",
      "        \"permissions\": [\n",
      "            \"MOVE_TO\",\n",
      "            \"EDIT_OBJECTIVE\",\n",
      "            \"VIEW_OBJECTIVE\",\n",
      "            \"CREATE_OBJECTIVE\",\n",
      "            \"MANAGE_PERMISSIONS\",\n",
      "            \"CREATE_KPI\",\n",
      "            \"EXPORT_CONTENT\",\n",
      "            \"DELETE_OBJECTIVE\"\n",
      "        ],\n",
      "        \"serviceName\": \"transformation-center\"\n",
      "    },\n",
      "    {\n",
      "        \"permissions\": [\n",
      "            \"DELETE\",\n",
      "            \"CREATE\",\n",
      "            \"GET\",\n",
      "            \"ADMIN\",\n",
      "            \"LIST\"\n",
      "        ],\n",
      "        \"serviceName\": \"storage-manager\"\n",
      "    },\n",
      "    {\n",
      "        \"permissions\": [\n",
      "            \"USE_ALL_DATA_MODELS\",\n",
      "            \"EDIT_ALL_DATA_POOLS\",\n",
      "            \"CREATE_DATA_POOL\"\n",
      "        ],\n",
      "        \"serviceName\": \"event-collection\"\n",
      "    },\n",
      "    {\n",
      "        \"permissions\": [\n",
      "            \"DELETE_SCHEDULERS\",\n",
      "            \"EDIT_SCHEDULERS\",\n",
      "            \"USE_ALL_SCHEDULERS\",\n",
      "            \"USE_ALL_APPS\",\n",
      "            \"CREATE_SCHEDULERS\",\n",
      "            \"MANAGE_ALL_APPS\",\n",
      "            \"CREATE_WORKSPACES\",\n",
      "            \"MANAGE_SCHEDULERS_PERMISSIONS\",\n",
      "            \"VIEW_CONFIGURATION\",\n",
      "            \"CREATE_APPS\",\n",
      "            \"MANAGE_ALL_MLFLOWS\",\n",
      "            \"CREATE_MLFLOWS\",\n",
      "            \"USE_ALL_MLFLOWS\",\n",
      "            \"MANAGE_ALL_WORKSPACES\"\n",
      "        ],\n",
      "        \"serviceName\": \"ml-workbench\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import duckdb\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pycelonis import get_celonis\n",
    "\n",
    "url = \"https://louis-herrmann-rwth-aachen-de.training.celonis.cloud\"\n",
    "api = \"NWE2NjdjOGEtYTkyMS00NDYyLTk0M2EtZjFiYjdhZDA5MTYzOmZJSDIydFd3TEwrQkUwV2tBVkhtN0N5VFI1aHdWYVJ2TDJVUWpoL2U5cUE4\"\n",
    "\n",
    "celonis = get_celonis(url, api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pool = celonis.pools.find(\"OCEL_Pool1\")\n",
    "data_model = data_pool.datamodels.find(\"OCEL_Model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table:\n",
    "    # should data model be a celonis data model or own class?\n",
    "    def __init__(self, name, data_model, data=None):\n",
    "        self.name = name\n",
    "        self.data_model = data_model\n",
    "        self.foreignKeys = {\"as_source\": [], \"as_target\": []}\n",
    "        for dictionary in data_model.foreign_keys.find_keys_by_source_name(name):\n",
    "            self.foreignKeys[\"as_source\"].append(dictionary)\n",
    "        for dictionary in data_model.foreign_keys.find_keys_by_target_name(name):\n",
    "            self.foreignKeys[\"as_target\"].append(dictionary)\n",
    "        \n",
    "        self.data = data\n",
    "    \n",
    "    def fetchData(self):\n",
    "        if self.data is None:\n",
    "            self.data = data_model.tables.find(self.name).get_data_frame()\n",
    "    \n",
    "    def getData(self):\n",
    "        if self.data is None:\n",
    "            self.fetchData()\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityTable(Table):\n",
    "    \n",
    "    # also make constructor with more parameters (e.g. passing columns...)\n",
    "    \n",
    "    def __init__(self, *args): # args: [name, data_model, data, actColumn, caseColumn, timestampColumn]\n",
    "        if len(args) < 2:\n",
    "            print(\"ActivityTable constructor needs more arguments\")\n",
    "        name = args[0]\n",
    "        data_model = args[1]\n",
    "        data = None\n",
    "        if len(args) >= 3:\n",
    "            data = args[2]\n",
    "        super().__init__(name, data_model, data)\n",
    "        \n",
    "        self.activity_column = None\n",
    "        self.timestamp_column = None\n",
    "        self.case_column = None\n",
    "                \n",
    "        self.caseTable = None\n",
    "        if len(args) <= 3:\n",
    "            activity_table = None\n",
    "            for table in data_model.process_configurations:\n",
    "                if table.activity_table.name == name:\n",
    "                    activity_table = table\n",
    "                    break\n",
    "            if activity_table:\n",
    "                self.activity_column = activity_table.activity_column\n",
    "                self.timestamp_column = activity_table.timestamp_column\n",
    "                self.case_column = activity_table.case_column\n",
    "                self.caseTable = activity_table.case_table.name\n",
    "        elif len(args) == 6:\n",
    "            # maybe create case table and set foreign key relation (can do this since we have data)\n",
    "            self.activity_column = args[3]\n",
    "            self.case_column = args[4]\n",
    "            self.timestamp_column = args[5]\n",
    "\n",
    "\n",
    "    def getData(self, shortened_columns=False):\n",
    "        if self.data is None:\n",
    "            self.fetchData()\n",
    "        if shortened_columns:\n",
    "            copy = self.data.copy(deep=True)\n",
    "            copy.rename(columns = {self.activity_column: \"ACTIVITY\", self.timestamp_column: \"TIMESTAMP\", self.case_column: \"OBJECTS\"}, inplace = True)\n",
    "            return copy\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tables:\n",
    "    def __init__(self):\n",
    "        self.collection = []\n",
    "    \n",
    "    def addTable(self, table):\n",
    "        self.collection.append(table)\n",
    "    \n",
    "    def removeTable(self, table):\n",
    "        self.collection.remove(table)\n",
    "\n",
    "    def removeByName(self, name):\n",
    "        for i in self.collection:\n",
    "            if i.name == name:\n",
    "                self.collection.remove(i)\n",
    "      \n",
    "    def output(self):\n",
    "        return [table.name for table in self.collection]\n",
    "\n",
    "    # returns first occurence\n",
    "    def find(self, name):\n",
    "        for table in self.collection:\n",
    "            if table.name == name:\n",
    "                return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModel:\n",
    "\n",
    "    def __init__(self, data_model):\n",
    "        self.data_model = data_model\n",
    "        \n",
    "        self.tables = Tables()\n",
    "        self.activity_tables = Tables()\n",
    "        \n",
    "        print(\"Fetching process configurations...\")\n",
    "        for table in data_model.process_configurations:\n",
    "            self.activity_tables.addTable(ActivityTable(table.activity_table.name, data_model))\n",
    "\n",
    "        print(\"Creating tables...\")\n",
    "        for table in data_model.tables:\n",
    "            act = self.activity_tables.find(table.name)\n",
    "            if act is not None:\n",
    "                self.tables.addTable(act)\n",
    "            else:        \n",
    "                self.tables.addTable(Table(table.name, data_model))\n",
    "        \n",
    "        print(\"Fetching foreign keys...\")\n",
    "        self.foreignKeys = data_model.foreign_keys\n",
    "        \n",
    "        print(\"Creating foreign key graph...\")\n",
    "        self.foreignKeyGraph = None\n",
    "\n",
    "        self.updateForeignKeyGraph()\n",
    "        \n",
    "        self.objectRelationships = {}\n",
    "        \n",
    "        # add other optimization attributes: \n",
    "            # all calculation methods should save result to avoid duplicate calculations\n",
    "        \n",
    "    def updateForeignKeyGraph(self):\n",
    "        graph = {}\n",
    "        for table in self.tables.collection:\n",
    "            connected_nodes = []\n",
    "            for dictionary in table.foreignKeys[\"as_source\"]:\n",
    "                connected_nodes.append(dictionary[\"target_table\"])\n",
    "            for dictionary in table.foreignKeys[\"as_target\"]:\n",
    "                connected_nodes.append(dictionary[\"source_table\"])\n",
    "            graph[table.name] = connected_nodes\n",
    "\n",
    "        G = nx.Graph()\n",
    "        for i in graph:\n",
    "            G.add_node(i)\n",
    "            for j in graph[i]:\n",
    "                G.add_edge(i, j)\n",
    "        \n",
    "        self.foreignKeyGraph = G\n",
    "    \n",
    "        \n",
    "    def calcForeignKeyPath(self, table1, table2):\n",
    "        try: \n",
    "            return nx.algorithms.shortest_paths.generic.shortest_path(self.foreignKeyGraph, source=table1, target=table2)\n",
    "        except:\n",
    "            print(\"No path found\")\n",
    "            return []\n",
    "\n",
    "            \n",
    "    # mergePath format: [{\"leftTable\": table1, \"leftColumn\": column1, \"rightTable\": table2, \"rightColumn\": column2}, {\"leftTable\": table3, \"leftColumn\": column3, \"rightTable\": table4, \"rightColumn\": column4}]\n",
    "    def calcMergePath(self, table1, table2):\n",
    "        path = self.calcForeignKeyPath(table1, table2)\n",
    "        \n",
    "        if len(path) == 0:\n",
    "            return\n",
    "        \n",
    "        potential_relations2 = self.tables.find(path[0]).foreignKeys[\"as_source\"] + self.tables.find(path[0]).foreignKeys[\"as_target\"]\n",
    "\n",
    "        mergePath = []\n",
    "        for i in range(len(path)-1):\n",
    "            potential_relations1 = potential_relations2\n",
    "            potential_relations2 = self.tables.find(path[i+1]).foreignKeys[\"as_source\"] + self.tables.find(path[i+1]).foreignKeys[\"as_target\"]\n",
    "            key_relation = {}\n",
    "            for relation in potential_relations1:\n",
    "                if relation in potential_relations2:\n",
    "                    key_relation = relation\n",
    "                    break\n",
    "            if key_relation == {}:\n",
    "                print(\"No Relation Found!\")\n",
    "                return mergePath\n",
    "            mergePath.append({\"leftTable\": key_relation[\"source_table\"], \"leftColumn\": key_relation[\"columns\"][0][0], \n",
    "                              \"rightTable\": key_relation[\"target_table\"], \"rightColumn\": key_relation[\"columns\"][0][1]})\n",
    "        return mergePath\n",
    "    \n",
    "    \n",
    "    def calcObjectRelationships(self, actTableName1, actTableName2):\n",
    "        if not self.isActivityTable(actTableName1) or not self.isActivityTable(actTableName2):\n",
    "            return\n",
    "        \n",
    "        if (actTableName1, actTableName2) in self.objectRelationships:\n",
    "            return self.objectRelationships[(actTableName1, actTableName2)]        \n",
    "        \n",
    "        actTable1 = self.activity_tables.find(actTableName1)\n",
    "        actTable2 = self.activity_tables.find(actTableName2)\n",
    "        \n",
    "        mergePath = self.calcMergePath(actTableName1, actTableName2)\n",
    "\n",
    "        df = actTable1.getData()\n",
    "        for relation in mergePath:\n",
    "            df = df.merge(self.tables.find(relation[\"leftTable\"]).getData() \\\n",
    "                .merge(self.tables.find(relation[\"rightTable\"]).getData(), \\\n",
    "                       left_on=relation[\"leftColumn\"], right_on=relation[\"rightColumn\"]))\n",
    "        \n",
    "        columns_to_keep = [actTable1.case_column, actTable2.case_column]\n",
    "        \n",
    "        df = df[list(set(columns_to_keep).intersection(df.columns))]\n",
    "        \n",
    "        df.drop_duplicates(inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        self.objectRelationships[(actTableName1, actTableName2)] = df\n",
    "        # save table as actual Table() object including foreign keys to activity tables...\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def groupEvents(self, actTableName):\n",
    "        if not self.isActivityTable(actTableName):\n",
    "            return\n",
    "        # group events in table with same timestamp + activity name\n",
    "        table = self.activity_tables.find(actTableName)\n",
    "        \n",
    "        grouped = self.tables.find(\"GROUP\" + \"{\" + actTableName + \"}\")\n",
    "        if grouped is not None:\n",
    "            return grouped\n",
    "        \n",
    "        events = {}\n",
    "\n",
    "        for ev in table.getData().to_dict(\"records\"):\n",
    "            evv = (ev[table.activity_column], ev[table.timestamp_column])\n",
    "            if evv not in events:\n",
    "                events[evv] = frozenset()\n",
    "            if isinstance(ev[table.case_column],frozenset):\n",
    "                events[evv] = events[evv].union(ev[table.case_column])\n",
    "            else:\n",
    "                events[evv] = events[evv].union(frozenset([ev[table.case_column]]))\n",
    "\n",
    "        df = []\n",
    "        \n",
    "        newName = \"GROUP\" + \"{\" + actTableName + \"}\"\n",
    "        for key, val in events.items():\n",
    "            df.append( {newName + \"_OBJECTS\": val, newName + \"_ACTIVITY\": key[0], newName + \"_TIMESTAMP\": key[1]} )\n",
    "        \n",
    "        # adding to tables/activityTables collection\n",
    "        actTable = ActivityTable(newName, self.data_model, \n",
    "                                 pd.DataFrame(df), newName + \"_ACTIVITY\", newName + \"_OBJECTS\", newName + \"_TIMESTAMP\")\n",
    "        self.tables.addTable(actTable)\n",
    "        self.activity_tables.addTable(actTable)\n",
    "        \n",
    "        self.createConnectionTable(table.name, actTable.name)\n",
    "        \n",
    "        # also group other attributes...\n",
    "        return actTable\n",
    "        \n",
    "\n",
    "\n",
    "    def union(self, actTableName1, actTableName2):\n",
    "        if not self.isActivityTable(actTableName1) or not self.isActivityTable(actTableName2):\n",
    "            return\n",
    "        \n",
    "        table1 = self.activity_tables.find(actTableName1)\n",
    "        table2 = self.activity_tables.find(actTableName2)\n",
    "        \n",
    "        union = self.tables.find(\"UNION_{\" + actTableName1 + \"}_AND_{\" + actTableName2 + \"}\")\n",
    "        if union is not None:\n",
    "            return union\n",
    "        \n",
    "        relationship = self.calcObjectRelationships(actTableName1, actTableName2)\n",
    "\n",
    "        df = table1.getData().merge(relationship, on=table1.case_column) \\\n",
    "                            .merge(table2.getData(), \n",
    "                                   left_on=[table2.case_column, table1.activity_column, table1.timestamp_column], \n",
    "                                   right_on=[table2.case_column, table2.activity_column, table2.timestamp_column])\n",
    "\n",
    "        \n",
    "        newDf2 = table2.getData()[[table2.case_column, table2.activity_column, table2.timestamp_column]].copy(deep=True)\n",
    "        newDf2.columns = [\"OBJECTS\", \"ACTIVITY\", \"TIMESTAMP\"]\n",
    "        \n",
    "        group = {}\n",
    "        \n",
    "        for row in df.to_dict(\"records\"):\n",
    "            key = (row[table1.case_column], row[table1.activity_column], row[table1.timestamp_column])\n",
    "            if key not in group:\n",
    "                group[key] = frozenset()\n",
    "            if isinstance(row[table2.case_column],frozenset):\n",
    "                group[key] = group[key].union(row[table2.case_column])\n",
    "            else:\n",
    "                group[key] = group[key].union([row[table2.case_column]])\n",
    "            i = newDf2[((newDf2.OBJECTS == row[table2.case_column]) &( newDf2.TIMESTAMP == key[2]) & (newDf2.ACTIVITY == key[1]))].index\n",
    "            newDf2.drop(i, inplace=True)\n",
    "            \n",
    "        newDf = table1.getData()[[table1.case_column, table1.activity_column, table1.timestamp_column]]\n",
    "        newDf.columns = [\"OBJECTS\", \"ACTIVITY\", \"TIMESTAMP\"]\n",
    "        \n",
    "        newDf = newDf.append(newDf2).reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        for index, row in newDf.iterrows():\n",
    "            key = (row[\"OBJECTS\"], row[\"ACTIVITY\"],row[\"TIMESTAMP\"])\n",
    "            if key in group:\n",
    "                if isinstance(newDf.at[index, \"OBJECTS\"], frozenset):\n",
    "                    newDf.at[index, \"OBJECTS\"] = newDf.at[index, \"OBJECTS\"].union(group[key])\n",
    "                else:\n",
    "                    newDf.at[index, \"OBJECTS\"] = frozenset([newDf.at[index, \"OBJECTS\"]]).union(group[key])\n",
    "            else:\n",
    "                # if no new objects will be added\n",
    "                if not isinstance(newDf.at[index, \"OBJECTS\"], frozenset):\n",
    "                    newDf.at[index, \"OBJECTS\"] = frozenset([newDf.at[index, \"OBJECTS\"]])\n",
    "\n",
    "        newDf = newDf.sort_values(by='TIMESTAMP').reset_index(drop=True)\n",
    "        \n",
    "        newName = \"UNION_{\" + actTableName1 + \"}_AND_{\" + actTableName2 + \"}\"\n",
    "        newDf.columns = [newName + \"_OBJECTS\", newName + \"_ACTIVITY\", newName + \"_TIMESTAMP\"]\n",
    "\n",
    "        activityTable = ActivityTable(newName, self.data_model, newDf, newName + \"_ACTIVITY\", newName + \"_OBJECTS\", newName + \"_TIMESTAMP\")\n",
    "        self.tables.addTable(activityTable)\n",
    "        self.activity_tables.addTable(activityTable)\n",
    "        \n",
    "        # create connection tables\n",
    "        for table in [table1, table2]:\n",
    "            self.createConnectionTable(table.name, activityTable.name)\n",
    "        \n",
    "        # what about other attributes?\n",
    "        \n",
    "        return activityTable\n",
    "\n",
    "    \n",
    "    def createConnectionTable(self, actTableName1, actTableName2):\n",
    "        if not self.isActivityTable(actTableName1) or not self.isActivityTable(actTableName2):\n",
    "            return\n",
    "        \n",
    "        table1 = self.activity_tables.find(actTableName1)\n",
    "        table2 = self.activity_tables.find(actTableName2)\n",
    "\n",
    "        unique = []\n",
    "        for obj in table1.getData()[table1.case_column]:\n",
    "            if obj not in unique:\n",
    "                unique.append(obj)\n",
    "\n",
    "        values = []\n",
    "        for i in range(len(unique)):\n",
    "            for j in table2.getData()[table2.case_column]:\n",
    "                if isinstance(unique[i],frozenset):\n",
    "                    u_set = unique[i]\n",
    "                else:\n",
    "                    u_set = frozenset([unique[i]])\n",
    "                if isinstance(j,frozenset):\n",
    "                    j_set = j\n",
    "                else:\n",
    "                    j_set = frozenset([j])\n",
    "                    \n",
    "                if u_set.intersection(j_set) != frozenset():\n",
    "                    values.append((j, unique[i]))\n",
    "        data = pd.DataFrame(values, columns=[table2.name + \"_COLUMN\", table1.name + \"_COLUMN\"])\n",
    "        data.drop_duplicates(inplace=True)\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "        newTable = Table(\"CONNECT_{\" + table2.name + \"}_AND_{\" + table1.name +\"}\", self.data_model, data)\n",
    "            \n",
    "        # add foreign keys\n",
    "        newTable.foreignKeys[\"as_target\"].append({'source_table': table2.name,\n",
    "                                                       'target_table': newTable.name,\n",
    "                                                       'columns': [(table2.case_column, table2.name + \"_COLUMN\")]})\n",
    "        table2.foreignKeys[\"as_source\"].append({'source_table': table2.name,\n",
    "                                                       'target_table': newTable.name,\n",
    "                                                       'columns': [(table2.case_column, table2.name + \"_COLUMN\")]})\n",
    "        newTable.foreignKeys[\"as_source\"].append({'source_table': newTable.name,\n",
    "                                                       'target_table': table1.name,\n",
    "                                                       'columns': [(table1.name + \"_COLUMN\", table1.case_column)]})\n",
    "        table1.foreignKeys[\"as_target\"].append({'source_table': newTable.name,\n",
    "                                                       'target_table': table1.name,\n",
    "                                                       'columns': [(table1.name + \"_COLUMN\", table1.case_column)]})\n",
    "        self.tables.addTable(newTable)\n",
    "        \n",
    "        self.updateForeignKeyGraph()\n",
    "        \n",
    "        return newTable\n",
    "    \n",
    "    \n",
    "    def isActivityTable(self, tableName):\n",
    "        if self.activity_tables.find(tableName) is not None:\n",
    "            return True\n",
    "        print(tableName + \" is not Activity Table\")\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def groupSpecificEvents(self, actTableName, events):\n",
    "        # instead of grouping entire table, only group specific events\n",
    "        # for now, events parameter is collection of events {o:o, a:a, t:t}, but later can expand based on filters...\n",
    "        # for filters, could for example leave object, actvity, or timestamp column of event empty and interpret as don't cares!\n",
    "        if not self.isActivityTable(actTableName):\n",
    "            return\n",
    "        \n",
    "        table = self.activity_tables.find(actTableName)\n",
    "\n",
    "        grouped = self.tables.find(\"GROUP_SPECIFIC\" + str(events) + \"{\" + actTableName + \"}\")\n",
    "        if grouped is not None:\n",
    "            return grouped\n",
    "        \n",
    "        data = table.getData(shortened_columns=True).copy(deep=True)[[\"OBJECTS\", \"ACTIVITY\", \"TIMESTAMP\"]]\n",
    "        grouped = {\"OBJECTS\": frozenset(), \"ACTIVITY\": frozenset()}\n",
    "        timestamps = []\n",
    "        \n",
    "        for event in events:\n",
    "                \n",
    "            indexes = []\n",
    "            for key, value in event.items():\n",
    "                i = data[((data[key] == value))].index\n",
    "                indexes.append(set(i))\n",
    "                \n",
    "            to_drop = set(indexes[0])\n",
    "            for i in indexes:\n",
    "                to_drop = to_drop.intersection(i)\n",
    "            to_drop = list(to_drop)\n",
    "            \n",
    "            for i in to_drop:\n",
    "                set_obj = data.at[i, \"OBJECTS\"]\n",
    "                if not isinstance(set_obj,frozenset):\n",
    "                    set_obj = frozenset([set_obj])\n",
    "                grouped[\"OBJECTS\"] = grouped[\"OBJECTS\"].union(set_obj)\n",
    "\n",
    "                set_act = data.at[i, \"ACTIVITY\"]\n",
    "                if not isinstance(set_act,frozenset):\n",
    "                    set_act = frozenset([set_act])\n",
    "                grouped[\"ACTIVITY\"] = grouped[\"ACTIVITY\"].union(set_act)\n",
    "\n",
    "                timestamps.append(data.at[i, \"TIMESTAMP\"])\n",
    "                \n",
    "            data.drop(to_drop, inplace=True)\n",
    "        \n",
    "        if len(timestamps) == 0:\n",
    "            print(\"None of the events were found in submitted table\")\n",
    "            return\n",
    "        \n",
    "        newName = \"GROUP_SPECIFIC\" + str(events) + \"{\" + actTableName + \"}\"\n",
    "\n",
    "        grouped[\"TIMESTAMP\"] = max(timestamps) # use latest timestamp as timestamp\n",
    "        data = data.append(grouped, ignore_index=True)\n",
    "        data = data.sort_values(by='TIMESTAMP').reset_index(drop=True)\n",
    "        data.rename(columns = {\"ACTIVITY\": newName + \"_ACTIVITY\", \"TIMESTAMP\": newName + \"_TIMESTAMP\", \"OBJECTS\": newName + \"_OBJECTS\"}, inplace = True)\n",
    "        \n",
    "        \n",
    "        # adding to tables/activityTables collection\n",
    "        actTable = ActivityTable(newName, self.data_model, \n",
    "                                 data, newName + \"_ACTIVITY\", newName + \"_OBJECTS\", newName + \"_TIMESTAMP\")\n",
    "        self.tables.addTable(actTable)\n",
    "        self.activity_tables.addTable(actTable)\n",
    "        \n",
    "        self.createConnectionTable(table.name, actTable.name)\n",
    "        \n",
    "        # also group other attributes...\n",
    "        return actTable\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching process configurations...\n"
     ]
    }
   ],
   "source": [
    "dm = DataModel(data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.tables.find(\"item_EVENTS\").getData(shortened_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [{\"ACTIVITY\": \"place order\"},\n",
    "         {\"OBJECTS\": \"item1\", \"ACTIVITY\": \"check availability\", \"TIMESTAMP\": pd.Timestamp('2020-07-09 08:21:00')}]\n",
    "dm.groupSpecificEvents(\"item_EVENTS\", events).getData(shortened_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unioned = dm.union(\"order_EVENTS\", \"item_EVENTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unioned.getData(shortened_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.calcObjectRelationships(unioned.name, \"item_EVENTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.union(unioned.name, \"order_EVENTS\").getData(shortened_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allUnion = dm.union(unioned.name, \"package_EVENTS\")\n",
    "allUnion.getData(shortened_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.groupEvents(allUnion.name).getData(shortened_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.groupEvents(\"item_EVENTS\").getData(shortened_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.union(dm.groupEvents(\"order_EVENTS\").name, dm.groupEvents(\"item_EVENTS\").name).getData(shortened_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.groupEvents(dm.union(\"order_EVENTS\", \"item_EVENTS\").name).getData(shortened_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "### ADD MORE COMMENT (better variable names, clean up...)\n",
    "### other attributes\n",
    "### everything based on names, base on tables instead?\n",
    "### currently only \"remembering\" tables based on name, can lead to very long and unideal names, especially e.g. for groupSpecificEvents where we use events as encoding. Use e.g. hash function instead?\n",
    "### create filter, similar like in groupSpecificEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
